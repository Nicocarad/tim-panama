{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report\n",
    ")\n",
    "import lightgbm as lgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLD = 5\n",
    "train_df = pd.read_parquet(\"train_df_slogan_lp.parquet\")\n",
    "val_df = pd.read_parquet(\"val_df_slogan_lp.parquet\")\n",
    "test_df = pd.read_parquet(\"test_df_slogan_lp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_with_guastocavo_labels = pd.read_csv(\n",
    "    \"20230101-20240101_real_time_clusters_filtered_guasto_cavo.csv\"\n",
    ")\n",
    "merge_train_df = pd.merge(\n",
    "    train_df, cluster_with_guastocavo_labels, on=\"cluster_id2\", how=\"left\"\n",
    ")\n",
    "merge_train_df.set_index(\"cluster_id2\", inplace=True)\n",
    "merge_val_df = pd.merge(\n",
    "    val_df, cluster_with_guastocavo_labels, on=\"cluster_id2\", how=\"left\"\n",
    ")\n",
    "merge_val_df.set_index(\"cluster_id2\", inplace=True)\n",
    "merge_test_df = pd.merge(\n",
    "    test_df, cluster_with_guastocavo_labels, on=\"cluster_id2\", how=\"left\"\n",
    ")\n",
    "merge_test_df.set_index(\"cluster_id2\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([merge_train_df, merge_val_df, merge_test_df])\n",
    "shuffled_df = merged_df.sample(frac=1, random_state=7)\n",
    "\n",
    "fold_size = len(shuffled_df) // 5\n",
    "\n",
    "fold1 = shuffled_df.iloc[:fold_size]\n",
    "fold2 = shuffled_df.iloc[fold_size:2*fold_size]\n",
    "fold3 = shuffled_df.iloc[2*fold_size:3*fold_size]\n",
    "fold4 = shuffled_df.iloc[3*fold_size:4*fold_size]\n",
    "fold5 = shuffled_df.iloc[4*fold_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_readable_metrics(avg_metrics):\n",
    "    print(\"Metriche Medie per Fold:\\n\")\n",
    "    print(\"{:<10} {:<12} {:<12} {:<12} {:<12}\".format('Label', 'Precision', 'Recall', 'F1-Score', 'Support'))\n",
    "    for label, metrics in avg_metrics.items():\n",
    "        if label != \"accuracy\":\n",
    "            print(\"{:<10} {:<12.2f} {:<12.2f} {:<12.2f} {:<12}\".format(\n",
    "                label,\n",
    "                metrics['precision'],\n",
    "                metrics['recall'],\n",
    "                metrics['f1-score'] ,\n",
    "                int(metrics['support'])\n",
    "            ))\n",
    "    print(\"\\nAccuracy: {:.2f}%\".format(avg_metrics[\"accuracy\"] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'False'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m sum_metrics[key]:\n\u001b[1;32m---> 31\u001b[0m             sum_metrics[key][metric] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mreport\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m[metric]\n\u001b[0;32m     32\u001b[0m     sum_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     34\u001b[0m avg_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     35\u001b[0m     key: {metric: total \u001b[38;5;241m/\u001b[39m NUM_FOLD \u001b[38;5;28;01mfor\u001b[39;00m metric, total \u001b[38;5;129;01min\u001b[39;00m sum_metrics[key]\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     37\u001b[0m }\n",
      "\u001b[1;31mKeyError\u001b[0m: 'False'"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=800, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    "\n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "\n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche Medie per Fold:\n",
      "\n",
      "Label      Precision    Recall       F1-Score     Support     \n",
      "False      0.59         0.20         0.29         133         \n",
      "True       0.85         0.97         0.90         609         \n",
      "\n",
      "Accuracy: 83.17%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5,fold6, fold7, fold8, fold9, fold10, fold11, fold12]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5, fold6, fold7, fold8, fold9, fold10, fold11, fold12][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "\n",
    "    clf = LogisticRegression(random_state=42, max_iter=200)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    " \n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "   \n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche Medie per Fold:\n",
      "\n",
      "Label      Precision    Recall       F1-Score     Support     \n",
      "False      0.58         0.21         0.31         133         \n",
      "True       0.85         0.97         0.90         609         \n",
      "\n",
      "Accuracy: 83.12%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5,fold6, fold7, fold8, fold9, fold10, fold11, fold12]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5, fold6, fold7, fold8, fold9, fold10, fold11, fold12][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "\n",
    "    clf = SVC(kernel=\"linear\", random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    " \n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "   \n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2449, number of negative: 523\n",
      "[LightGBM] [Info] Total Bins 516\n",
      "[LightGBM] [Info] Number of data points in the train set: 2972, number of used features: 258\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.824024 -> initscore=1.543854\n",
      "[LightGBM] [Info] Start training from score 1.543854\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2423, number of negative: 548\n",
      "[LightGBM] [Info] Total Bins 540\n",
      "[LightGBM] [Info] Number of data points in the train set: 2971, number of used features: 270\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.815550 -> initscore=1.486486\n",
      "[LightGBM] [Info] Start training from score 1.486486\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2436, number of negative: 535\n",
      "[LightGBM] [Info] Total Bins 526\n",
      "[LightGBM] [Info] Number of data points in the train set: 2971, number of used features: 263\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.819926 -> initscore=1.515846\n",
      "[LightGBM] [Info] Start training from score 1.515846\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2450, number of negative: 521\n",
      "[LightGBM] [Info] Total Bins 516\n",
      "[LightGBM] [Info] Number of data points in the train set: 2971, number of used features: 258\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.824638 -> initscore=1.548093\n",
      "[LightGBM] [Info] Start training from score 1.548093\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2430, number of negative: 541\n",
      "[LightGBM] [Info] Total Bins 546\n",
      "[LightGBM] [Info] Number of data points in the train set: 2971, number of used features: 273\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.817906 -> initscore=1.502227\n",
      "[LightGBM] [Info] Start training from score 1.502227\n",
      "Metriche Medie per Fold:\n",
      "\n",
      "Label      Precision    Recall       F1-Score     Support     \n",
      "False      0.51         0.36         0.42         133         \n",
      "True       0.87         0.93         0.90         609         \n",
      "\n",
      "Accuracy: 82.39%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# Definizione dei parametri\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"force_row_wise\": True,\n",
    "}\n",
    "\n",
    "num_round = 2000\n",
    "\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5,fold6, fold7, fold8, fold9, fold10, fold11, fold12]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5, fold6, fold7, fold8, fold9, fold10, fold11, fold12][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    \n",
    "    bst = lgb.train(params, train_data, num_round)\n",
    "    y_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "    y_val_pred = np.round(y_pred)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    " \n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "   \n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
