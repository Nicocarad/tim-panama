{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report\n",
    ")\n",
    "import lightgbm as lgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLD = 5\n",
    "train_df = pd.read_parquet(\"train_df_link_lp.parquet\")\n",
    "val_df = pd.read_parquet(\"val_df_link_lp.parquet\")\n",
    "test_df = pd.read_parquet(\"test_df_link_lp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_with_guastocavo_labels = pd.read_csv(\n",
    "    \"20230101-20240101_real_time_clusters_filtered_guasto_cavo.csv\"\n",
    ")\n",
    "merge_train_df = pd.merge(\n",
    "    train_df, cluster_with_guastocavo_labels, on=\"cluster_id2\", how=\"left\"\n",
    ")\n",
    "merge_train_df.set_index(\"cluster_id2\", inplace=True)\n",
    "merge_val_df = pd.merge(\n",
    "    val_df, cluster_with_guastocavo_labels, on=\"cluster_id2\", how=\"left\"\n",
    ")\n",
    "merge_val_df.set_index(\"cluster_id2\", inplace=True)\n",
    "merge_test_df = pd.merge(\n",
    "    test_df, cluster_with_guastocavo_labels, on=\"cluster_id2\", how=\"left\"\n",
    ")\n",
    "merge_test_df.set_index(\"cluster_id2\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([merge_train_df, merge_val_df, merge_test_df])\n",
    "shuffled_df = merged_df.sample(frac=1, random_state=7)\n",
    "\n",
    "fold1 = shuffled_df.iloc[: len(shuffled_df) // NUM_FOLD]\n",
    "fold2 = shuffled_df.iloc[\n",
    "    len(shuffled_df) // NUM_FOLD : 2 * len(shuffled_df) // NUM_FOLD\n",
    "]\n",
    "fold3 = shuffled_df.iloc[\n",
    "    2 * len(shuffled_df) // NUM_FOLD : 3 * len(shuffled_df) // NUM_FOLD\n",
    "]\n",
    "fold4 = shuffled_df.iloc[\n",
    "    3 * len(shuffled_df) // NUM_FOLD : 4 * len(shuffled_df) // NUM_FOLD\n",
    "]\n",
    "fold5 = shuffled_df.iloc[4 * len(shuffled_df) // NUM_FOLD :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_readable_metrics(avg_metrics):\n",
    "    print(\"Metriche Medie per Fold:\\n\")\n",
    "    print(\"{:<10} {:<12} {:<12} {:<12} {:<12}\".format('Label', 'Precision', 'Recall', 'F1-Score', 'Support'))\n",
    "    for label, metrics in avg_metrics.items():\n",
    "        if label != \"accuracy\":\n",
    "            print(\"{:<10} {:<12.2f} {:<12.2f} {:<12.2f} {:<12}\".format(\n",
    "                label,\n",
    "                metrics['precision'],\n",
    "                metrics['recall'],\n",
    "                metrics['f1-score'] ,\n",
    "                int(metrics['support'])\n",
    "            ))\n",
    "    print(\"\\nAccuracy: {:.2f}%\".format(avg_metrics[\"accuracy\"] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche Medie per Fold:\n",
      "\n",
      "Label      Precision    Recall       F1-Score     Support     \n",
      "False      0.56         0.27         0.37         253         \n",
      "True       0.85         0.95         0.90         1081        \n",
      "\n",
      "Accuracy: 82.06%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=400, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    "\n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "\n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche Medie per Fold:\n",
      "\n",
      "Label      Precision    Recall       F1-Score     Support     \n",
      "False      0.61         0.17         0.26         253         \n",
      "True       0.83         0.97         0.90         1081        \n",
      "\n",
      "Accuracy: 82.18%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "\n",
    "    clf = LogisticRegression(random_state=42, max_iter=200)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    " \n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "   \n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche Medie per Fold:\n",
      "\n",
      "Label      Precision    Recall       F1-Score     Support     \n",
      "False      0.57         0.13         0.21         253         \n",
      "True       0.83         0.98         0.90         1081        \n",
      "\n",
      "Accuracy: 81.54%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "\n",
    "    clf = SVC(kernel=\"linear\", random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    " \n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "   \n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4318, number of negative: 1021\n",
      "[LightGBM] [Info] Total Bins 686\n",
      "[LightGBM] [Info] Number of data points in the train set: 5339, number of used features: 343\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.808766 -> initscore=1.442010\n",
      "[LightGBM] [Info] Start training from score 1.442010\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4320, number of negative: 1018\n",
      "[LightGBM] [Info] Total Bins 692\n",
      "[LightGBM] [Info] Number of data points in the train set: 5338, number of used features: 346\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.809292 -> initscore=1.445415\n",
      "[LightGBM] [Info] Start training from score 1.445415\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4327, number of negative: 1012\n",
      "[LightGBM] [Info] Total Bins 698\n",
      "[LightGBM] [Info] Number of data points in the train set: 5339, number of used features: 349\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.810451 -> initscore=1.452946\n",
      "[LightGBM] [Info] Start training from score 1.452946\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 1008\n",
      "[LightGBM] [Info] Total Bins 680\n",
      "[LightGBM] [Info] Number of data points in the train set: 5338, number of used features: 340\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.811165 -> initscore=1.457599\n",
      "[LightGBM] [Info] Start training from score 1.457599\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4325, number of negative: 1013\n",
      "[LightGBM] [Info] Total Bins 686\n",
      "[LightGBM] [Info] Number of data points in the train set: 5338, number of used features: 343\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.810229 -> initscore=1.451496\n",
      "[LightGBM] [Info] Start training from score 1.451496\n",
      "Metriche Medie per Fold:\n",
      "\n",
      "Label      Precision    Recall       F1-Score     Support     \n",
      "False      0.52         0.32         0.40         253         \n",
      "True       0.85         0.93         0.89         1081        \n",
      "\n",
      "Accuracy: 81.45%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# Definizione dei parametri\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"force_row_wise\": True,\n",
    "}\n",
    "\n",
    "num_round = 2000\n",
    "\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    \n",
    "    bst = lgb.train(params, train_data, num_round)\n",
    "    y_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "    y_val_pred = np.round(y_pred)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    " \n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "   \n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIM_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
