{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report\n",
    ")\n",
    "import lightgbm as lgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLD = 5\n",
    "train_df = pd.read_parquet(\"train_df_link_lp.parquet\")\n",
    "val_df = pd.read_parquet(\"val_df_link_lp.parquet\")\n",
    "test_df = pd.read_parquet(\"test_df_link_lp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_with_guastocavo_labels = pd.read_csv(\n",
    "    \"20230101-20240101_real_time_clusters_filtered_guasto_cavo.csv\"\n",
    ")\n",
    "merge_train_df = pd.merge(\n",
    "    train_df, cluster_with_guastocavo_labels, on=\"cluster_id2\", how=\"left\"\n",
    ")\n",
    "merge_train_df.set_index(\"cluster_id2\", inplace=True)\n",
    "merge_val_df = pd.merge(\n",
    "    val_df, cluster_with_guastocavo_labels, on=\"cluster_id2\", how=\"left\"\n",
    ")\n",
    "merge_val_df.set_index(\"cluster_id2\", inplace=True)\n",
    "merge_test_df = pd.merge(\n",
    "    test_df, cluster_with_guastocavo_labels, on=\"cluster_id2\", how=\"left\"\n",
    ")\n",
    "merge_test_df.set_index(\"cluster_id2\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([merge_train_df, merge_val_df, merge_test_df])\n",
    "shuffled_df = merged_df.sample(frac=1, random_state=7)\n",
    "\n",
    "fold1 = shuffled_df.iloc[: len(shuffled_df) // NUM_FOLD]\n",
    "fold2 = shuffled_df.iloc[\n",
    "    len(shuffled_df) // NUM_FOLD : 2 * len(shuffled_df) // NUM_FOLD\n",
    "]\n",
    "fold3 = shuffled_df.iloc[\n",
    "    2 * len(shuffled_df) // NUM_FOLD : 3 * len(shuffled_df) // NUM_FOLD\n",
    "]\n",
    "fold4 = shuffled_df.iloc[\n",
    "    3 * len(shuffled_df) // NUM_FOLD : 4 * len(shuffled_df) // NUM_FOLD\n",
    "]\n",
    "fold5 = shuffled_df.iloc[4 * len(shuffled_df) // NUM_FOLD :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_readable_metrics(avg_metrics):\n",
    "    print(\"Metriche Medie per Fold:\\n\")\n",
    "    print(\"{:<10} {:<12} {:<12} {:<12} {:<12}\".format('Label', 'Precision', 'Recall', 'F1-Score', 'Support'))\n",
    "    for label, metrics in avg_metrics.items():\n",
    "        if label != \"accuracy\":\n",
    "            print(\"{:<10} {:<12.2f} {:<12.2f} {:<12.2f} {:<12}\".format(\n",
    "                label,\n",
    "                metrics['precision'],\n",
    "                metrics['recall'],\n",
    "                metrics['f1-score'] ,\n",
    "                int(metrics['support'])\n",
    "            ))\n",
    "    print(\"\\nAccuracy: {:.2f}%\".format(avg_metrics[\"accuracy\"] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche Medie per Fold:\n",
      "\n",
      "Label      Precision    Recall       F1-Score     Support     \n",
      "False      0.63         0.33         0.43         133         \n",
      "True       0.87         0.96         0.91         603         \n",
      "\n",
      "Accuracy: 84.24%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    "\n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "\n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche Medie per Fold:\n",
      "\n",
      "Label      Precision    Recall       F1-Score     Support     \n",
      "False      0.64         0.21         0.32         133         \n",
      "True       0.85         0.97         0.91         603         \n",
      "\n",
      "Accuracy: 83.59%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "\n",
    "    clf = LogisticRegression(random_state=42, max_iter=200)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    " \n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "   \n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche Medie per Fold:\n",
      "\n",
      "Label      Precision    Recall       F1-Score     Support     \n",
      "False      0.62         0.23         0.33         133         \n",
      "True       0.85         0.97         0.90         603         \n",
      "\n",
      "Accuracy: 83.35%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "\n",
    "    clf = SVC(kernel=\"linear\", random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    " \n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "   \n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2409, number of negative: 536\n",
      "[LightGBM] [Info] Total Bins 518\n",
      "[LightGBM] [Info] Number of data points in the train set: 2945, number of used features: 259\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.817997 -> initscore=1.502833\n",
      "[LightGBM] [Info] Start training from score 1.502833\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2419, number of negative: 526\n",
      "[LightGBM] [Info] Total Bins 512\n",
      "[LightGBM] [Info] Number of data points in the train set: 2945, number of used features: 256\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.821392 -> initscore=1.525808\n",
      "[LightGBM] [Info] Start training from score 1.525808\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2406, number of negative: 539\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 2945, number of used features: 255\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.816978 -> initscore=1.496005\n",
      "[LightGBM] [Info] Start training from score 1.496005\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2420, number of negative: 525\n",
      "[LightGBM] [Info] Total Bins 532\n",
      "[LightGBM] [Info] Number of data points in the train set: 2945, number of used features: 266\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.821732 -> initscore=1.528125\n",
      "[LightGBM] [Info] Start training from score 1.528125\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2410, number of negative: 534\n",
      "[LightGBM] [Info] Total Bins 504\n",
      "[LightGBM] [Info] Number of data points in the train set: 2944, number of used features: 252\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.818614 -> initscore=1.506986\n",
      "[LightGBM] [Info] Start training from score 1.506986\n",
      "Metriche Medie per Fold:\n",
      "\n",
      "Label      Precision    Recall       F1-Score     Support     \n",
      "False      0.54         0.32         0.40         133         \n",
      "True       0.86         0.94         0.90         603         \n",
      "\n",
      "Accuracy: 82.72%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# Definizione dei parametri\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"force_row_wise\": True,\n",
    "}\n",
    "\n",
    "num_round = 1000\n",
    "\n",
    "\n",
    "for iter in range(NUM_FOLD):\n",
    "\n",
    "    train_dfs = [\n",
    "        fold for i, fold in enumerate([fold1, fold2, fold3, fold4, fold5]) if i != iter\n",
    "    ]\n",
    "    val_df = [fold1, fold2, fold3, fold4, fold5][iter]\n",
    "\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    X_train = train_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_train = train_df[\"GUASTO CAVO\"].values.ravel()\n",
    "\n",
    "    X_val = val_df.drop(columns=[\"GUASTO CAVO\"])\n",
    "    y_val = val_df[\"GUASTO CAVO\"]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    \n",
    "    bst = lgb.train(params, train_data, num_round)\n",
    "    y_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "    y_val_pred = np.round(y_pred)\n",
    "    results.append(classification_report(y_val, y_val_pred, output_dict=True))\n",
    " \n",
    "sum_metrics = {\n",
    "    key: {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "sum_metrics[\"accuracy\"] = 0\n",
    "   \n",
    "for report in results:\n",
    "    for key in [\"False\", \"True\"]:\n",
    "        for metric in sum_metrics[key]:\n",
    "            sum_metrics[key][metric] += report[key][metric]\n",
    "    sum_metrics[\"accuracy\"] += report[\"accuracy\"]\n",
    "\n",
    "avg_metrics = {\n",
    "    key: {metric: total / NUM_FOLD for metric, total in sum_metrics[key].items()}\n",
    "    for key in [\"False\", \"True\"]\n",
    "}\n",
    "avg_metrics[\"accuracy\"] = sum_metrics[\"accuracy\"] / NUM_FOLD\n",
    "\n",
    "print_readable_metrics(avg_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIM_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
