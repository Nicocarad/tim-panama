{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noc_roma = pd.read_csv(\"../NOC_ROMA_202403110800-202403111600_clusters_la.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero Cluster: 35\n",
      "Numero di cluster corretti: 13\n",
      "Numero di cluster totalmente errati: 12\n",
      "Numero di cluster parzialmente corretti: 10\n"
     ]
    }
   ],
   "source": [
    "noc_roma_grouped = noc_roma.groupby(\"cluster_id\")\n",
    "numero_cluster = noc_roma[\"cluster_id\"].nunique()\n",
    "\n",
    "cluster_corretti = []\n",
    "cluster_totalmente_errati = []\n",
    "cluster_parzialmente_corretti = []\n",
    "\n",
    "for cluster_id, gruppo in noc_roma.groupby(\"cluster_id\"):\n",
    "    if gruppo[\"NOTE SPV\"].str.contains(\"CORRELAZIONE OK\").all():\n",
    "        cluster_corretti.append(cluster_id)\n",
    "    elif gruppo[\"NOTE SPV\"].str.contains(\"CORRELAZIONE ERRATA\").all():\n",
    "        cluster_totalmente_errati.append(cluster_id)\n",
    "    else:\n",
    "        cluster_parzialmente_corretti.append(cluster_id)\n",
    "\n",
    "print(\"Numero Cluster:\", numero_cluster)\n",
    "print(f\"Numero di cluster corretti: {len(cluster_corretti)}\")\n",
    "print(f\"Numero di cluster totalmente errati: {len(cluster_totalmente_errati)}\")\n",
    "print(f\"Numero di cluster parzialmente corretti: {len(cluster_parzialmente_corretti)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single contamination Forest value IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Assicurati che \"first_occurrence\" sia in formato datetime\n",
    "noc_roma[\"first_occurrence\"] = pd.to_datetime(noc_roma[\"first_occurrence\"])\n",
    "\n",
    "outlier_labels = []\n",
    "df_list = []\n",
    "\n",
    "for cluster_id, group in noc_roma.groupby(\"cluster_id\"):\n",
    "    # Convertire i tempi in numeri\n",
    "    X = group[[\"first_occurrence\"]].apply(lambda x: x.astype(np.int64) // 10**9).values.reshape(-1, 1)\n",
    "    \n",
    "    # Applicazione dell'Isolation Forest\n",
    "    isolation_forest = IsolationForest(contamination=\"auto\", random_state=42)\n",
    "    labels = isolation_forest.fit_predict(X)\n",
    "    \n",
    "    # Adattamento dei valori per essere coerenti con LOF (1 per normali, -1 per outlier)\n",
    "    outlier_labels.extend(labels)\n",
    "    \n",
    "    # Creazione di un DataFrame temporaneo con i risultati\n",
    "    temp_df = pd.DataFrame({\n",
    "        \"cluster_id\": cluster_id,\n",
    "        \"OUTLIER_LABEL\": labels,\n",
    "    })\n",
    "    \n",
    "    df_list.append(temp_df)\n",
    "\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "filtered_noc_roma = noc_roma[[\"NOTE SPV\"]]\n",
    "\n",
    "# Merge dei risultati con il DataFrame originale\n",
    "merge_df = pd.merge(final_df, filtered_noc_roma, left_index=True, right_index=True, how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di cluster corretti ricostruiti correttamente: 3 di 13\n",
      "Numero di cluster corretti ricostruiti correttamente: 2 di 10\n"
     ]
    }
   ],
   "source": [
    "merge_grouped = merge_df.groupby(\"cluster_id\")\n",
    "\n",
    "reconstructed_correct_clusters = 0\n",
    "for cluster_id, group in merge_grouped:\n",
    "\n",
    "    if cluster_id in cluster_corretti:\n",
    "        if group[\"OUTLIER_LABEL\"].nunique() == 1:\n",
    "            reconstructed_correct_clusters += 1\n",
    "\n",
    "print(\n",
    "    f\"Numero di cluster corretti ricostruiti correttamente: {reconstructed_correct_clusters} di {len(cluster_corretti)}\"\n",
    ")\n",
    "\n",
    "\n",
    "partially_correct_clusters_predicted = 0\n",
    "\n",
    "for cluster_id, group in merge_grouped:\n",
    "\n",
    "    if cluster_id in cluster_parzialmente_corretti:\n",
    "        # Filtra gli allarmi con \"CORRELAZIONE OK\"\n",
    "        allarmi_ok = group[group[\"NOTE SPV\"].str.contains(\"CORRELAZIONE OK\")]\n",
    "        # Filtra gli allarmi con \"CORRELAZIONE ERRATA\"\n",
    "        allarmi_errata = group[\n",
    "            group[\"NOTE SPV\"].str.contains(\"CORRELAZIONE ERRATA\")\n",
    "        ]\n",
    "\n",
    "        # Verifica che tutti gli allarmi OK abbiano la stessa label\n",
    "        label_unica_ok = allarmi_ok[\"OUTLIER_LABEL\"].nunique() == 1\n",
    "\n",
    "        # Verifica che gli allarmi ERRATA abbiano una label diversa da quella degli allarmi OK\n",
    "        if not allarmi_errata.empty and label_unica_ok:\n",
    "            label_ok = allarmi_ok[\"OUTLIER_LABEL\"].iloc[0]\n",
    "            # Verifica che tutte le label ERRATA siano diverse da quella OK\n",
    "            label_diverse_errata = (\n",
    "                not allarmi_errata[\"OUTLIER_LABEL\"].isin([label_ok]).any()\n",
    "            )\n",
    "\n",
    "            if label_diverse_errata:\n",
    "                partially_correct_clusters_predicted += 1\n",
    "\n",
    "print(\n",
    "    f\"Numero di cluster corretti ricostruiti correttamente: {partially_correct_clusters_predicted} di {len(cluster_parzialmente_corretti)}\"\n",
    ")\n",
    "\n",
    "merge_df.to_excel(\"IsolationForest_df.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple contamination value IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valori di contamination :  84%|████████▎ | 41/49 [02:34<00:31,  3.93s/it]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "contamination_values = np.arange(0.01, 0.5, 0.01)\n",
    "\n",
    "\n",
    "results = []\n",
    "reconstructed_correct_clusters_values = []\n",
    "partially_correct_clusters_predicted_values = []\n",
    "cluster_corretti_values = []\n",
    "cluster_parzialmente_corretti_values = []\n",
    "\n",
    "for n in tqdm(contamination_values, desc=\"Valori di contamination \"):\n",
    "    OUTLIER_LABELs = []\n",
    "    df_list = []\n",
    "\n",
    "    for cluster_id, group in noc_roma.groupby(\"cluster_id\"):\n",
    "        # Convertire i tempi in numeri\n",
    "        X = (\n",
    "            group[[\"first_occurrence\"]]\n",
    "            .apply(lambda x: x.astype(np.int64) // 10**9)\n",
    "            .values.reshape(-1, 1)\n",
    "        )\n",
    "\n",
    "        # Applicazione dell'Isolation Forest\n",
    "        isolation_forest = IsolationForest(contamination=n, random_state=42)\n",
    "        labels = isolation_forest.fit_predict(X)\n",
    "\n",
    "        # Adattamento dei valori per essere coerenti con LOF (1 per normali, -1 per outlier)\n",
    "        outlier_labels.extend(labels)\n",
    "\n",
    "        # Creazione di un DataFrame temporaneo con i risultati\n",
    "        temp_df = pd.DataFrame(\n",
    "            {\n",
    "                \"cluster_id\": cluster_id,\n",
    "                \"OUTLIER_LABEL\": labels,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df_list.append(temp_df)\n",
    "\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    filtered_noc_roma = noc_roma[[\"NOTE SPV\"]]\n",
    "\n",
    "    merge_df = pd.merge(\n",
    "        final_df, filtered_noc_roma, left_index=True, right_index=True, how=\"left\"\n",
    "    )\n",
    "\n",
    "    merge_grouped = merge_df.groupby(\"cluster_id\")\n",
    "\n",
    "    predict_correct_clusters = 0\n",
    "    for cluster_id, group in merge_grouped:\n",
    "\n",
    "        if cluster_id in cluster_corretti:\n",
    "            if group[\"OUTLIER_LABEL\"].nunique() == 1:\n",
    "                predict_correct_clusters += 1\n",
    "\n",
    "    reconstructed_correct_clusters_values.append(predict_correct_clusters)\n",
    "    cluster_corretti_values.append(len(cluster_corretti))\n",
    "\n",
    "    predict_wrong_clusters = 0\n",
    "\n",
    "    for cluster_id, group in merge_grouped:\n",
    "\n",
    "        if cluster_id in cluster_parzialmente_corretti:\n",
    "            if cluster_id in cluster_parzialmente_corretti:\n",
    "                # Filtra gli allarmi con \"CORRELAZIONE OK\"\n",
    "                allarmi_ok = group[group[\"NOTE SPV\"].str.contains(\"CORRELAZIONE OK\")]\n",
    "                # Filtra gli allarmi con \"CORRELAZIONE ERRATA\"\n",
    "                allarmi_errata = group[\n",
    "                    group[\"NOTE SPV\"].str.contains(\"CORRELAZIONE ERRATA\")\n",
    "                ]\n",
    "\n",
    "                # Verifica che tutti gli allarmi OK abbiano la stessa label\n",
    "                label_unica_ok = allarmi_ok[\"OUTLIER_LABEL\"].nunique() == 1\n",
    "\n",
    "                # Verifica che gli allarmi ERRATA abbiano una label diversa da quella degli allarmi OK\n",
    "                if not allarmi_errata.empty and label_unica_ok:\n",
    "                    label_ok = allarmi_ok[\"OUTLIER_LABEL\"].iloc[0]\n",
    "                    # Verifica che tutte le label ERRATA siano diverse da quella OK\n",
    "                    label_diverse_errata = (\n",
    "                        not allarmi_errata[\"OUTLIER_LABEL\"].isin([label_ok]).any()\n",
    "                    )\n",
    "\n",
    "                    if label_diverse_errata:\n",
    "                        predict_wrong_clusters += 1\n",
    "    partially_correct_clusters_predicted_values.append(predict_wrong_clusters)\n",
    "    cluster_parzialmente_corretti_values.append(len(cluster_parzialmente_corretti))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "plt.plot(contamination_values, partially_correct_clusters_predicted_values, label=\"Cluster Parzialmente Corretti Predetti\")\n",
    "plt.plot(\n",
    "    contamination_values, reconstructed_correct_clusters_values, label=\"Cluster Corretti Ricostruiti\"\n",
    ")\n",
    "plt.plot(contamination_values, cluster_corretti_values, label=\"Totale Cluster Corretti\")\n",
    "plt.plot(\n",
    "    contamination_values,\n",
    "    cluster_parzialmente_corretti_values,\n",
    "    label=\"Totale Cluster Parzialmente Corretti\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Epsilon(secondi)\")\n",
    "plt.ylabel(\"# Cluster\")\n",
    "plt.title(\"Valori al Variare di Epsilon\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
